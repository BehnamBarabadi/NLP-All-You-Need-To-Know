{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings transform a one-hot encoded vector (a vector that is 0 in elements except one, which is 1) into a much smaller dimension vector of real numbers. The one-hot encoded vector is also known as a sparse vector, whilst the real valued vector is known as a dense vector.\n",
    "\n",
    "The key concept in these word embeddings is that words that appear in similar contexts appear nearby in the vector space, i.e. the Euclidean distance between these two word vectors is small.\n",
    "\n",
    "https://github.com/spro/practical-pytorch/blob/master/glove-word-vectors/glove-word-vectors.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GloVe vectors\n",
    "First, we'll load the GloVe vectors. The name field specifies what the vectors have been trained on, here the 6B means a corpus of 6 billion words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400000 words in the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "# 6 Billion words used to train, each word is represented with dimensionality of 100\n",
    "\n",
    "print(f'There are {len(glove.itos)} words in the vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In these set of GloVe vectors, every single word is lower-case only.\n",
    "- 400,000 words, each represented by a vector of 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below implies that row 0 is the vector associated with the word 'the', row 1 for ',' (comma), row 2 for '.' (period), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " ',',\n",
       " '.',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '-',\n",
       " 'that',\n",
       " 'on',\n",
       " 'is']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 15 words in GloVe vocabulary\n",
    "glove.itos[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric index of given words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36623"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi['dazzle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166593"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi['behnam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector representation of a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3169,  0.0307, -0.0961,  0.2953, -0.0797, -0.6273,  0.2274, -0.2679,\n",
       "         0.1378, -0.0654,  0.1710,  0.7600, -0.3771, -0.2558,  0.2803,  0.0839,\n",
       "        -0.0527,  0.1139,  0.0614,  0.0408, -0.4747,  0.4947,  0.2939,  0.1276,\n",
       "        -0.9034,  0.5451, -0.5878,  0.0788,  0.1740,  0.1525,  0.2043, -0.8871,\n",
       "         0.0424, -0.0807,  0.2236, -0.8022,  0.2143,  0.3548,  0.2322, -0.1880,\n",
       "        -0.0302,  0.2244,  0.5588,  0.5244,  0.0565,  0.0974,  0.2689,  0.6710,\n",
       "        -0.0384,  0.6108,  0.0954, -0.1304, -0.0603, -0.3533, -0.1242,  0.5028,\n",
       "        -0.3027,  0.2162, -0.7939, -0.6337, -0.1156, -0.6282,  0.1086, -0.0864,\n",
       "        -0.5286,  0.1353, -0.0608, -0.3036, -0.0048,  0.3765,  0.2213,  0.6235,\n",
       "         0.6105, -0.8010,  0.2631, -0.4587, -0.1978,  0.1259,  0.0366,  0.3063,\n",
       "         0.0573, -0.0450,  0.0377,  0.1521,  0.5279,  0.1083,  0.2403, -0.1780,\n",
       "        -0.1725,  0.0634,  0.1030,  0.0447,  0.4559, -0.0587,  0.0996,  0.0363,\n",
       "         0.1626,  0.5560, -0.7709,  0.1101])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors[glove.stoi['behnam']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we'll create a function that takes in word embeddings and a word and returns the associated vector. It'll also throw an error if the word doesn't exist in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(embeddings, word):\n",
    "    \n",
    "    assert word in embeddings.stoi, f'*{word}* is not in the vocab!'\n",
    "    \n",
    "    return embeddings.vectors[embeddings.stoi[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3169,  0.0307, -0.0961,  0.2953, -0.0797, -0.6273,  0.2274, -0.2679,\n",
       "         0.1378, -0.0654,  0.1710,  0.7600, -0.3771, -0.2558,  0.2803,  0.0839,\n",
       "        -0.0527,  0.1139,  0.0614,  0.0408, -0.4747,  0.4947,  0.2939,  0.1276,\n",
       "        -0.9034,  0.5451, -0.5878,  0.0788,  0.1740,  0.1525,  0.2043, -0.8871,\n",
       "         0.0424, -0.0807,  0.2236, -0.8022,  0.2143,  0.3548,  0.2322, -0.1880,\n",
       "        -0.0302,  0.2244,  0.5588,  0.5244,  0.0565,  0.0974,  0.2689,  0.6710,\n",
       "        -0.0384,  0.6108,  0.0954, -0.1304, -0.0603, -0.3533, -0.1242,  0.5028,\n",
       "        -0.3027,  0.2162, -0.7939, -0.6337, -0.1156, -0.6282,  0.1086, -0.0864,\n",
       "        -0.5286,  0.1353, -0.0608, -0.3036, -0.0048,  0.3765,  0.2213,  0.6235,\n",
       "         0.6105, -0.8010,  0.2631, -0.4587, -0.1978,  0.1259,  0.0366,  0.3063,\n",
       "         0.0573, -0.0450,  0.0377,  0.1521,  0.5279,  0.1083,  0.2403, -0.1780,\n",
       "        -0.1725,  0.0634,  0.1030,  0.0447,  0.4559, -0.0587,  0.0996,  0.0363,\n",
       "         0.1626,  0.5560, -0.7709,  0.1101])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector(glove, 'behnam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to find the words similar to a certain input word, we first find the vector of this input word, then we scan through our vocabulary finding any vectors similar to this input word vector.\n",
    "\n",
    "The function below returns the closest 6 words to an input word vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(embeddings, vector, n = 6):\n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    for neighbor in embeddings.itos: # iterates trough all 400,000 words\n",
    "        # (each word in vocab, distance of each word in vocab with the input word)\n",
    "        distances.append((neighbor, torch.dist(vector, get_vector(embeddings, neighbor))))\n",
    "    \n",
    "    return sorted(distances, key = lambda x: x[1])[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5941, -0.2980,  0.4845, -1.0484, -0.1234, -0.7606, -0.3824,  0.0547,\n",
       "        -0.5145,  0.4720,  1.1300, -0.6752,  0.5322, -1.4753,  0.7268, -1.0578,\n",
       "         0.7085, -0.0226,  0.6492, -0.5271,  0.5378, -1.3725,  0.6805,  0.3684,\n",
       "         0.3946,  0.3264,  0.7379,  0.3787,  1.4240,  0.8743, -0.9630, -0.2236,\n",
       "        -0.2297,  0.4366,  0.1340,  0.7371, -0.6388,  0.0802, -0.4933,  0.5903,\n",
       "        -0.1161,  0.1407,  0.2577, -0.6073,  0.1068, -0.4910, -0.5081,  0.0193,\n",
       "        -0.8991,  0.5141, -0.6505,  0.0244, -0.4841,  0.4380,  1.1311, -0.9572,\n",
       "         0.2540,  0.0479, -1.1566,  0.1825,  0.1217, -0.2933, -0.9783,  0.0429,\n",
       "        -0.1324, -1.1153, -0.1110, -0.4143, -0.7511,  0.3366,  0.0281, -0.4760,\n",
       "        -1.0994,  1.0847,  0.1064,  0.0838, -0.3123,  0.3206, -0.6370,  0.4812,\n",
       "         0.0880, -0.9473,  0.8987, -0.4394,  0.5618, -0.5753, -0.1427,  0.1543,\n",
       "         0.3749, -0.0160, -0.0100, -0.1311, -0.1834,  1.4182, -0.0115, -0.5173,\n",
       "         0.2775,  1.1060,  0.6482, -0.1146])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector(glove, 'ufc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ufc', tensor(0.)),\n",
       " ('wec', tensor(4.1984)),\n",
       " ('strikeforce', tensor(5.0995)),\n",
       " ('wcw', tensor(5.1773)),\n",
       " ('ecw', tensor(5.3557)),\n",
       " ('wbc', tensor(5.3616))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(glove, get_vector(glove, 'ufc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('behnam', tensor(0.)),\n",
       " ('khedr', tensor(2.8868)),\n",
       " ('bahnam', tensor(2.8913)),\n",
       " ('bottinelli', tensor(2.9203)),\n",
       " ('samuela', tensor(2.9273)),\n",
       " ('shongwe', tensor(2.9329))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(glove, get_vector(glove, 'behnam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shenanigans', tensor(0.)),\n",
       " ('chicanery', tensor(2.3785)),\n",
       " ('hijinks', tensor(2.6764)),\n",
       " ('escapades', tensor(2.7821)),\n",
       " ('machinations', tensor(2.8699)),\n",
       " ('gamesmanship', tensor(2.9044))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(glove, get_vector(glove, 'shenanigans'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll also create another function that will nicely print out the tuples returned by our closest function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tuples(tuples):\n",
    "    \n",
    "    for t in tuples:\n",
    "        print('(%.4f) %s' % (t[1], t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0000) ronaldo\n",
      "(3.1684) ronaldinho\n",
      "(3.2217) rivaldo\n",
      "(3.2934) beckham\n",
      "(3.4223) cristiano\n",
      "(3.4708) robinho\n"
     ]
    }
   ],
   "source": [
    "print_tuples(closest(glove, get_vector(glove, 'ronaldo')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogies\n",
    "\n",
    "with a well-trained word vector space certain semantic relationships  can be captured with regular vector arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(embeddings, w1, w2, w3, n = 6):\n",
    "    \n",
    "    print('\\n[%s : %s :: %s : ?]' % (w1, w2, w3))\n",
    "   \n",
    "    closest_words = closest(embeddings, \\\n",
    "                            get_vector(embeddings, w2)\n",
    "                          - get_vector(embeddings, w1) \\\n",
    "                          + get_vector(embeddings, w3), \\\n",
    "                            n + 3) # we add 3 to get rid of w1, w2, w3 later\n",
    " \n",
    "    closest_words = [x for x in closest_words if x[0] not in [w1, w2, w3]][:n]\n",
    "        \n",
    "    return closest_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "night - moon ==  sun + ?\n",
    "- day or morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[moon : night :: sun : ?]\n",
      "(5.7069) morning\n",
      "(5.7276) afternoon\n",
      "(5.8023) evening\n",
      "(6.1410) hours\n",
      "(6.2797) saturday\n",
      "(6.3056) sunday\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'moon', 'night', 'sun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[fly : bird :: swim : ?]\n",
      "(5.9754) swimming\n",
      "(6.2409) shark\n",
      "(6.4822) dolphin\n",
      "(6.5421) whale\n",
      "(6.6276) cat\n",
      "(6.6457) gorilla\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'fly', 'bird', 'swim'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interesting failure mode\n",
    "- GloVe detecs sun as a name and tries to complete it with another name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[earth : moon :: sun : ?]\n",
      "(6.2294) lee\n",
      "(6.4125) kang\n",
      "(6.4644) tan\n",
      "(6.4757) yang\n",
      "(6.4853) lin\n",
      "(6.5220) chong\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'earth', 'moon', 'sun')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem with the previous code was that sun doesn't have any moon like earth does.\n",
    "- We change the sun with jupiter which has moons named: io, ganymede, europa, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[earth : moon :: jupiter : ?]\n",
      "(5.7522) io\n",
      "(5.9079) moons\n",
      "(5.9303) ganymede\n",
      "(6.2325) saturn\n",
      "(6.2599) neptune\n",
      "(6.2854) uranus\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'earth', 'moon', 'jupiter')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ufc : submission :: boxing : ?]\n",
      "(6.4599) courts\n",
      "(6.6362) storytelling\n",
      "(6.6554) mind\n",
      "(6.6827) hand\n",
      "(6.7017) tradition\n",
      "(6.7031) practice\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'ufc', 'submission', 'boxing')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
